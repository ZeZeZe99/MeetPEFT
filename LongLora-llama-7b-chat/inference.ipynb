{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from llama_attn_replace import replace_llama_attn\n",
    "import deepspeed\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSpeed Offloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        }\n",
    "    }, \n",
    "    \"train_batch_size\": 1,\n",
    "    'weight_quantization': {\n",
    "        'quantized_initialization' : {\n",
    "            'num_bits': 4,\n",
    "            'group_size': 64,\n",
    "            \"group_dim\": 1,\n",
    "            \"symmetric\": False\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input_llama2\": (\n",
    "        \"<s>[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\"\n",
    "        \"<</SYS>> \\n\\n {instruction} [/INST]\"\n",
    "    ),\n",
    "    \"prompt_llama2\": \"[INST]{instruction}[/INST]\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"Yukang/LongAlpaca-7B\"\n",
    "model_name = \"merged/continue-qm-16k\"\n",
    "cache_dir = \"./cache\"\n",
    "context_size = 16384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config, Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_llama_attn(inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and context_size > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(context_size / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a6763081ef40ff9464a2431b63a661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096, padding_idx=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(32001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 21:29:43,006] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown\n",
      "[2023-12-04 21:29:43,007] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-04 21:29:43,007] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2023-12-04 21:29:43,128] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.31.17.147, master_port=29500\n",
      "[2023-12-04 21:29:43,129] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2023-12-04 21:29:43,976] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-12-04 21:29:43,978] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload\n",
      "[2023-12-04 21:29:44,118] [INFO] [utils.py:795:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-12-04 21:29:44,119] [INFO] [utils.py:796:see_memory_usage] MA 3.65 GB         Max_MA 3.65 GB         CA 3.79 GB         Max_CA 4 GB \n",
      "[2023-12-04 21:29:44,120] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 3.11 GB, percent = 10.0%\n",
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
      "[2023-12-04 21:29:46,391] [INFO] [utils.py:795:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-12-04 21:29:46,392] [INFO] [utils.py:796:see_memory_usage] MA 0.13 GB         Max_MA 3.65 GB         CA 3.79 GB         Max_CA 4 GB \n",
      "[2023-12-04 21:29:46,393] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 7.79 GB, percent = 25.1%\n",
      "[2023-12-04 21:29:46,394] [INFO] [config.py:979:print] DeepSpeedEngine configuration:\n",
      "[2023-12-04 21:29:46,395] [INFO] [config.py:983:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-12-04 21:29:46,395] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-12-04 21:29:46,396] [INFO] [config.py:983:print]   amp_enabled .................. False\n",
      "[2023-12-04 21:29:46,396] [INFO] [config.py:983:print]   amp_params ................... False\n",
      "[2023-12-04 21:29:46,397] [INFO] [config.py:983:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-12-04 21:29:46,397] [INFO] [config.py:983:print]   bfloat16_enabled ............. False\n",
      "[2023-12-04 21:29:46,398] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-12-04 21:29:46,398] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-12-04 21:29:46,398] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-12-04 21:29:46,399] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa93bcd5210>\n",
      "[2023-12-04 21:29:46,399] [INFO] [config.py:983:print]   communication_data_type ...... None\n",
      "[2023-12-04 21:29:46,401] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-12-04 21:29:46,401] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False\n",
      "[2023-12-04 21:29:46,402] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False\n",
      "[2023-12-04 21:29:46,402] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-12-04 21:29:46,402] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False\n",
      "[2023-12-04 21:29:46,403] [INFO] [config.py:983:print]   dataloader_drop_last ......... False\n",
      "[2023-12-04 21:29:46,403] [INFO] [config.py:983:print]   disable_allgather ............ False\n",
      "[2023-12-04 21:29:46,404] [INFO] [config.py:983:print]   dump_state ................... False\n",
      "[2023-12-04 21:29:46,404] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-12-04 21:29:46,404] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False\n",
      "[2023-12-04 21:29:46,405] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-12-04 21:29:46,405] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-12-04 21:29:46,406] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-12-04 21:29:46,406] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-12-04 21:29:46,406] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-12-04 21:29:46,407] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-12-04 21:29:46,409] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False\n",
      "[2023-12-04 21:29:46,409] [INFO] [config.py:983:print]   elasticity_enabled ........... False\n",
      "[2023-12-04 21:29:46,410] [INFO] [config.py:983:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-12-04 21:29:46,410] [INFO] [config.py:983:print]   fp16_auto_cast ............... None\n",
      "[2023-12-04 21:29:46,411] [INFO] [config.py:983:print]   fp16_enabled ................. False\n",
      "[2023-12-04 21:29:46,411] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-12-04 21:29:46,412] [INFO] [config.py:983:print]   global_rank .................. 0\n",
      "[2023-12-04 21:29:46,412] [INFO] [config.py:983:print]   grad_accum_dtype ............. None\n",
      "[2023-12-04 21:29:46,412] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 1\n",
      "[2023-12-04 21:29:46,413] [INFO] [config.py:983:print]   gradient_clipping ............ 0.0\n",
      "[2023-12-04 21:29:46,413] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-12-04 21:29:46,414] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-12-04 21:29:46,414] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-12-04 21:29:46,414] [INFO] [config.py:983:print]   load_universal_checkpoint .... False\n",
      "[2023-12-04 21:29:46,415] [INFO] [config.py:983:print]   loss_scale ................... 0\n",
      "[2023-12-04 21:29:46,415] [INFO] [config.py:983:print]   memory_breakdown ............. False\n",
      "[2023-12-04 21:29:46,416] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False\n",
      "[2023-12-04 21:29:46,416] [INFO] [config.py:983:print]   mics_shard_size .............. -1\n",
      "[2023-12-04 21:29:46,416] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-12-04 21:29:46,417] [INFO] [config.py:983:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-12-04 21:29:46,420] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-12-04 21:29:46,420] [INFO] [config.py:983:print]   optimizer_name ............... None\n",
      "[2023-12-04 21:29:46,421] [INFO] [config.py:983:print]   optimizer_params ............. None\n",
      "[2023-12-04 21:29:46,421] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2023-12-04 21:29:46,422] [INFO] [config.py:983:print]   pld_enabled .................. False\n",
      "[2023-12-04 21:29:46,422] [INFO] [config.py:983:print]   pld_params ................... False\n",
      "[2023-12-04 21:29:46,422] [INFO] [config.py:983:print]   prescale_gradients ........... False\n",
      "[2023-12-04 21:29:46,423] [INFO] [config.py:983:print]   scheduler_name ............... None\n",
      "[2023-12-04 21:29:46,423] [INFO] [config.py:983:print]   scheduler_params ............. None\n",
      "[2023-12-04 21:29:46,423] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2023-12-04 21:29:46,424] [INFO] [config.py:983:print]   sparse_attention ............. None\n",
      "[2023-12-04 21:29:46,424] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False\n",
      "[2023-12-04 21:29:46,424] [INFO] [config.py:983:print]   steps_per_print .............. 10\n",
      "[2023-12-04 21:29:46,425] [INFO] [config.py:983:print]   train_batch_size ............. 1\n",
      "[2023-12-04 21:29:46,426] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-12-04 21:29:46,427] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False\n",
      "[2023-12-04 21:29:46,427] [INFO] [config.py:983:print]   use_node_local_storage ....... False\n",
      "[2023-12-04 21:29:46,428] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False\n",
      "[2023-12-04 21:29:46,428] [INFO] [config.py:983:print]   weight_quantization_config ... q_type='symmetric' q_groups=1 enabled=True num_bits=8 quantized_initialization={'num_bits': 4, 'group_size': 64, 'group_dim': 1, 'symmetric': False} post_init_quant={}\n",
      "[2023-12-04 21:29:46,429] [INFO] [config.py:983:print]   world_size ................... 1\n",
      "[2023-12-04 21:29:46,429] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False\n",
      "[2023-12-04 21:29:46,430] [INFO] [config.py:983:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-12-04 21:29:46,430] [INFO] [config.py:983:print]   zero_enabled ................. True\n",
      "[2023-12-04 21:29:46,430] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-12-04 21:29:46,431] [INFO] [config.py:983:print]   zero_optimization_stage ...... 3\n",
      "[2023-12-04 21:29:46,431] [INFO] [config.py:969:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"train_batch_size\": 1, \n",
      "    \"weight_quantization\": {\n",
      "        \"quantized_initialization\": {\n",
      "            \"num_bits\": 4, \n",
      "            \"group_size\": 64, \n",
      "            \"group_dim\": 1, \n",
      "            \"symmetric\": false\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# run if using deepspeed\n",
    "model_engine, _, _, _ = deepspeed.initialize(config_params=ds_config, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if using deepspeed\n",
    "model_engine = model_engine.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        model_max_length=context_size if context_size > orig_ctx_len else orig_ctx_len,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(\n",
    "    model, tokenizer, temperature=0.6, top_p=0.9, max_gen_len=512):\n",
    "    def response(prompt):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "        #streamer = TextStreamer(tokenizer)\n",
    "        \n",
    "        # run if using deepspeed\n",
    "        output = model.module.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_gen_len,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=1.1\n",
    "            #streamer=streamer,\n",
    "        )\n",
    "        \n",
    "        # output = model.generate(\n",
    "        #     **inputs,\n",
    "        #     max_new_tokens=max_gen_len,\n",
    "        #     temperature=temperature,\n",
    "        #     top_p=top_p,\n",
    "        #     use_cache=use_cache,\n",
    "        #     #streamer=streamer,\n",
    "        # )\n",
    "        \n",
    "        out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        out = out.split(prompt.lstrip(\"<s>\"))[1].strip()\n",
    "        out = out.split(\"</s>\")[0].strip()\n",
    "        return out\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if using deepspeed\n",
    "respond = build_generator(model_engine, tokenizer, max_gen_len=512)\n",
    "# respond = build_generator(model, tokenizer, max_gen_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_no_input = PROMPT_DICT[\"prompt_llama2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Summarize the meeting transcript in two sentences.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_inference_mb(question, in_file, out_file):\n",
    "    with open(in_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for meeting in tqdm(data):\n",
    "            with torch.no_grad():\n",
    "                sample_result = {}\n",
    "                id = meeting[\"id\"]\n",
    "                material = meeting[\"source\"]\n",
    "                target = meeting[\"summary\"]\n",
    "                prompt = prompt_no_input.format_map({\"instruction\": material + \"\\n%s\"%question})\n",
    "                output = respond(prompt=prompt)\n",
    "                sample_result[\"target\"] = re.sub(r'\\n', '', target)\n",
    "                sample_result[\"prediction\"] = re.sub(r'\\n', '', output)\n",
    "                print(sample_result[\"prediction\"])\n",
    "                sample_result[\"id\"] = id\n",
    "                json.dump(sample_result, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "            del sample_result, material, target, prompt, output, id\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"../MeetingBank/test_segment_16k_sample_100.json\"\n",
    "out_file = \"./output/output_MB_16k.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "  0%|          | 0/100 [03:37<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m file_inference_mb(question, in_file, out_file)\n",
      "\u001b[1;32m/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m target \u001b[39m=\u001b[39m meeting[\u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m prompt \u001b[39m=\u001b[39m prompt_no_input\u001b[39m.\u001b[39mformat_map({\u001b[39m\"\u001b[39m\u001b[39minstruction\u001b[39m\u001b[39m\"\u001b[39m: material \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39mquestion})\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m output \u001b[39m=\u001b[39m respond(prompt\u001b[39m=\u001b[39;49mprompt)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m sample_result[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m sample_result[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, output)\n",
      "\u001b[1;32m/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb Cell 25\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#streamer = TextStreamer(tokenizer)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# run if using deepspeed\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmodule\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mmax_gen_len,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m#streamer=streamer,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# output = model.generate(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#     **inputs,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#     max_new_tokens=max_gen_len,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#     #streamer=streamer,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-152-39-212.compute-1.amazonaws.com/home/ubuntu/MeetPEFT/LongLora-llama-7b-chat/inference.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m out \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1653\u001b[0m         input_ids,\n\u001b[1;32m   1654\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1655\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1656\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1657\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1658\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1659\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1660\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1661\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1662\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1663\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1664\u001b[0m     )\n\u001b[1;32m   1666\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2733\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2736\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2737\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2738\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2739\u001b[0m )\n\u001b[1;32m   2741\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2742\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1038\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1037\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1039\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1040\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1041\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1042\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1043\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1044\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1045\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1046\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1047\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1048\u001b[0m )\n\u001b[1;32m   1050\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    922\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    926\u001b[0m         hidden_states,\n\u001b[1;32m    927\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    928\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    929\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    930\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    931\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    932\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    935\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:635\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    632\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    634\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    636\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    637\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    638\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    639\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    640\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    641\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    642\u001b[0m     padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    643\u001b[0m )\n\u001b[1;32m    644\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    646\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/MeetPEFT/LongLora-llama-7b-chat/llama_attn_replace.py:382\u001b[0m, in \u001b[0;36mforward_flashattn_inference\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    379\u001b[0m bsz, q_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[1;32m    380\u001b[0m kv_heads \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnum_key_value_heads\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\n\u001b[0;32m--> 382\u001b[0m q, k, v \u001b[39m=\u001b[39m (\n\u001b[1;32m    383\u001b[0m     op(hidden_states)\u001b[39m.\u001b[39mview(bsz, q_len, nh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    384\u001b[0m     \u001b[39mfor\u001b[39;00m op, nh \u001b[39min\u001b[39;00m (\n\u001b[1;32m    385\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads),\n\u001b[1;32m    386\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj, kv_heads),\n\u001b[1;32m    387\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj, kv_heads),\n\u001b[1;32m    388\u001b[0m     )\n\u001b[1;32m    389\u001b[0m )\n\u001b[1;32m    390\u001b[0m \u001b[39m# shape: (b, s, num_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    392\u001b[0m kv_seq_len \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/MeetPEFT/LongLora-llama-7b-chat/llama_attn_replace.py:383\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    379\u001b[0m bsz, q_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[1;32m    380\u001b[0m kv_heads \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnum_key_value_heads\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\n\u001b[1;32m    382\u001b[0m q, k, v \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 383\u001b[0m     op(hidden_states)\u001b[39m.\u001b[39mview(bsz, q_len, nh, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    384\u001b[0m     \u001b[39mfor\u001b[39;00m op, nh \u001b[39min\u001b[39;00m (\n\u001b[1;32m    385\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads),\n\u001b[1;32m    386\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj, kv_heads),\n\u001b[1;32m    387\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj, kv_heads),\n\u001b[1;32m    388\u001b[0m     )\n\u001b[1;32m    389\u001b[0m )\n\u001b[1;32m    390\u001b[0m \u001b[39m# shape: (b, s, num_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    392\u001b[0m kv_seq_len \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1557\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1553\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1554\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00margs_kwargs_result\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1555\u001b[0m             )\n\u001b[1;32m   1556\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1557\u001b[0m     args_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args)\n\u001b[1;32m   1558\u001b[0m     \u001b[39mif\u001b[39;00m args_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1559\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args_result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:15\u001b[0m, in \u001b[0;36minstrument_w_nvtx.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     14\u001b[0m     get_accelerator()\u001b[39m.\u001b[39mrange_push(func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     ret_val \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     16\u001b[0m     get_accelerator()\u001b[39m.\u001b[39mrange_pop()\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m ret_val\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py:392\u001b[0m, in \u001b[0;36mDeepSpeedZeRoOffload._register_hooks_recursively.<locals>._pre_forward_module_hook\u001b[0;34m(module, *args)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39m@instrument_w_nvtx\u001b[39m\n\u001b[1;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pre_forward_module_hook\u001b[39m(module, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 392\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_sub_module_forward_function(module)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py:505\u001b[0m, in \u001b[0;36mDeepSpeedZeRoOffload.pre_sub_module_forward_function\u001b[0;34m(self, sub_module)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m param_coordinator\u001b[39m.\u001b[39mis_record_trace():\n\u001b[1;32m    504\u001b[0m     param_coordinator\u001b[39m.\u001b[39mrecord_module(sub_module)\n\u001b[0;32m--> 505\u001b[0m param_coordinator\u001b[39m.\u001b[39;49mfetch_sub_module(sub_module, forward\u001b[39m=\u001b[39;49mprev_grad_state)\n\u001b[1;32m    506\u001b[0m torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad_state)\n\u001b[1;32m    507\u001b[0m see_memory_usage(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBefore sub module function \u001b[39m\u001b[39m{\u001b[39;00msub_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m after fetch\u001b[39m\u001b[39m\"\u001b[39m, force\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:15\u001b[0m, in \u001b[0;36minstrument_w_nvtx.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     14\u001b[0m     get_accelerator()\u001b[39m.\u001b[39mrange_push(func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     ret_val \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     16\u001b[0m     get_accelerator()\u001b[39m.\u001b[39mrange_pop()\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m ret_val\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py:297\u001b[0m, in \u001b[0;36mPartitionedParameterCoordinator.fetch_sub_module\u001b[0;34m(self, current_submodule, forward)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__inflight_param_registry:\n\u001b[1;32m    296\u001b[0m     wait_numel \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m param\u001b[39m.\u001b[39mpartition_numel()\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mwith\u001b[39;00m get_accelerator()\u001b[39m.\u001b[39mstream(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__allgather_stream):\n\u001b[1;32m    298\u001b[0m         \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__ongoing_fetch_events \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__ongoing_fetch_events[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mquery():\n\u001b[1;32m    299\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__ongoing_fetch_events\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/deepspeed/accelerator/real_accelerator.py:51\u001b[0m, in \u001b[0;36mget_accelerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_current_accelerator_supported\u001b[39m():\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m get_accelerator() \u001b[39min\u001b[39;00m SUPPORTED_ACCELERATOR_LIST\n\u001b[0;32m---> 51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_accelerator\u001b[39m():\n\u001b[1;32m     52\u001b[0m     \u001b[39mglobal\u001b[39;00m ds_accelerator\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m ds_accelerator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_inference_mb(question, in_file, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
