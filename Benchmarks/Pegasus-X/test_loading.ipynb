{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training, validation and test data from MeetingBank json files\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load training data\n",
    "with open('/home/ubuntu/MeetPEFT/MeetingBank/train_segment_16k.json') as f:\n",
    "    train_split = json.load(f)\n",
    "with open('/home/ubuntu/MeetPEFT/MeetingBank/validation_segment_16k.json') as f:\n",
    "    validation_split = json.load(f)\n",
    "with open('/home/ubuntu/MeetPEFT/MeetingBank/test_segment_16k.json') as f:\n",
    "    test_split = json.load(f)\n",
    "\n",
    "print(\"finished loading json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_data(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data_list = json.load(f)\n",
    "    \n",
    "    print(len(data_list))\n",
    "        \n",
    "    # Initialize a dictionary to hold reformatted data\n",
    "    print(data_list[0].keys())\n",
    "    reformatted_data = {key: [] for key in data_list[0].keys()}\n",
    "\n",
    "    # Iterate over each data point and aggregate values by column\n",
    "    for data_point in data_list:\n",
    "        for key in reformatted_data.keys():\n",
    "            reformatted_data[key].append(data_point[key])\n",
    "    \n",
    "    return reformatted_data\n",
    "\n",
    "# Load and reformat the data\n",
    "train_split = reformat_data('/home/ubuntu/MeetPEFT/MeetingBank/train_segment_16k.json')\n",
    "validation_split = reformat_data('/home/ubuntu/MeetPEFT/MeetingBank/validation_segment_16k.json')\n",
    "test_split = reformat_data('/home/ubuntu/MeetPEFT/MeetingBank/test_segment_16k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine them into a datasets object\n",
    "# combine them into a datasets object\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_dict(train_split),\n",
    "    'validation': datasets.Dataset.from_dict(validation_split),\n",
    "    'test': datasets.Dataset.from_dict(test_split)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "train_split = dataset['train']\n",
    "validation_split = dataset['validation']\n",
    "test_split = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference without fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PegasusXForConditionalGeneration\n",
    "\n",
    "model = PegasusXForConditionalGeneration.from_pretrained(\"google/pegasus-x-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-x-large\")\n",
    "\n",
    "\n",
    "for sample in test_split['source']:\n",
    "    inputs = tokenizer(sample, max_length=16384, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "    result = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data_split):\n",
    "  for instance in data_split:\n",
    "    yield instance['id'], instance['summary'], instance['transcript']\n",
    "\n",
    "# create generators\n",
    "train_generator = generator(train_split)\n",
    "val_generator = generator(validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, PegasusModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-x-large\")\n",
    "model = PegasusModel.from_pretrained(\"google/pegasus-x-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"source\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=16384, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=1024, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataloader for training\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
