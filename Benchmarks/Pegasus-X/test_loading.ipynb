{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading json files\n"
     ]
    }
   ],
   "source": [
    "# load training, validation and test data from MeetingBank json files\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load training data\n",
    "with open('MeetingBank/train_text.json') as f:\n",
    "    train_split = json.load(f)\n",
    "with open('MeetingBank/validation_text.json') as f:\n",
    "    validation_split = json.load(f)\n",
    "with open('MeetingBank/test_text.json') as f:\n",
    "    test_split = json.load(f)\n",
    "\n",
    "print(\"finished loading json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MeetingBank/train_segment_16k.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m reformatted_data\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Load and reformat the data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m train_split \u001b[39m=\u001b[39m reformat_data(\u001b[39m'\u001b[39;49m\u001b[39mMeetingBank/train_segment_16k.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m validation_split \u001b[39m=\u001b[39m reformat_data(\u001b[39m'\u001b[39m\u001b[39mMeetingBank/validation_segment_16k.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m test_split \u001b[39m=\u001b[39m reformat_data(\u001b[39m'\u001b[39m\u001b[39mMeetingBank/test_segment_16k.json\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreformat_data\u001b[39m(json_file):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(json_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         data_list \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qingyangliu/Desktop/11667/MeetPEFT/Benchmarks/Pegasus-X/test_loading.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(data_list))\n",
      "File \u001b[0;32m~/anaconda3/envs/meetpeft/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MeetingBank/train_segment_16k.json'"
     ]
    }
   ],
   "source": [
    "def reformat_data(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data_list = json.load(f)\n",
    "    \n",
    "    print(len(data_list))\n",
    "        \n",
    "    # Initialize a dictionary to hold reformatted data\n",
    "    print(data_list[0].keys())\n",
    "    reformatted_data = {key: [] for key in data_list[0].keys()}\n",
    "\n",
    "    # Iterate over each data point and aggregate values by column\n",
    "    for data_point in data_list:\n",
    "        for key in reformatted_data.keys():\n",
    "            reformatted_data[key].append(data_point[key])\n",
    "    \n",
    "    return reformatted_data\n",
    "\n",
    "# Load and reformat the data\n",
    "train_split = reformat_data('MeetingBank/train_segment_16k.json')\n",
    "validation_split = reformat_data('MeetingBank/validation_segment_16k.json')\n",
    "test_split = reformat_data('MeetingBank/test_segment_16k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine them into a datasets object\n",
    "# combine them into a datasets object\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_dict(train_split),\n",
    "    'validation': datasets.Dataset.from_dict(validation_split),\n",
    "    'test': datasets.Dataset.from_dict(test_split)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'summary', 'source_length'],\n",
      "        num_rows: 1587\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['source', 'summary', 'source_length'],\n",
      "        num_rows: 198\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source', 'summary', 'source_length'],\n",
      "        num_rows: 199\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "train_split = dataset['train']\n",
    "validation_split = dataset['validation']\n",
    "test_split = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data_split):\n",
    "  for instance in data_split:\n",
    "    yield instance['id'], instance['summary'], instance['transcript']\n",
    "\n",
    "# create generators\n",
    "train_generator = generator(train_split)\n",
    "val_generator = generator(validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98d9c44db21428fa9c1e9f4ef6b5032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db23c76f3134dd6b2822bcc401e27bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load pegasus model\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1205e0a0ab3542e9ad02f3ea61169141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a7f397ef9a40bfbe7432b6ab2ef3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616667b504b3409abf67528dbcba002b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/6.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbd4b9216684b2ab48e5d4786ffb0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bba4e23f9e46969904b260f0ae79ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type pegasus_x to instantiate a model of type pegasus. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2369da01674400aaed548b233fa1c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusModel were not initialized from the model checkpoint at google/pegasus-x-large and are newly initialized: ['model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.encoder_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.11.encoder_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn.v_proj.bias', 'model.encoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.13.encoder_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.12.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.13.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.decoder.embed_positions.weight', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.encoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.15.encoder_attn.v_proj.bias', 'model.decoder.layers.13.encoder_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.12.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.14.self_attn.out_proj.bias', 'model.encoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn.out_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.12.encoder_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.14.encoder_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.k_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn.q_proj.bias', 'model.decoder.layers.13.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.k_proj.bias', 'model.decoder.layers.14.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.15.encoder_attn.out_proj.bias', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.encoder_attn.k_proj.bias', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.8.encoder_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.13.encoder_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.14.encoder_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.15.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.15.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.15.encoder_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.encoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PegasusModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-x-large\")\n",
    "model = PegasusModel.from_pretrained(\"google/pegasus-x-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"source\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=16384, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=1024, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c02ec853184777814edae485e18707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1587 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cea92325f64d8cab29ced32183cdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638b919b73004c61bf36e4a87ad9f3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataloader for training\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
