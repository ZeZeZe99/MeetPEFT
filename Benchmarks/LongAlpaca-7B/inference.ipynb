{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:12:41,335] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from llama_attn_replace import replace_llama_attn\n",
    "import deepspeed\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSpeed Offloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        }\n",
    "    }, \n",
    "    \"train_batch_size\": 1,\n",
    "    'weight_quantization': {\n",
    "        'quantized_initialization' : {\n",
    "            'num_bits': 4,\n",
    "            'group_size': 64,\n",
    "            \"group_dim\": 1,\n",
    "            \"symmetric\": False\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input_llama2\": (\n",
    "        \"<s>[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\"\n",
    "        \"<</SYS>> \\n\\n {instruction} [/INST]\"\n",
    "    ),\n",
    "    \"prompt_llama2\": \"[INST]{instruction}[/INST]\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Yukang/LongAlpaca-7B\"\n",
    "cache_dir = \"./cache\"\n",
    "context_size = 32768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config, Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_llama_attn(inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and context_size > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(context_size / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/pytorch did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6199953a65ee4857b2885ec9b484da94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096, padding_idx=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(32001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:12:48,517] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown\n",
      "[2023-12-04 06:12:48,518] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-12-04 06:12:48,518] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2023-12-04 06:12:48,626] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.31.18.216, master_port=29500\n",
      "[2023-12-04 06:12:48,627] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2023-12-04 06:12:49,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-12-04 06:12:49,428] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload\n",
      "[2023-12-04 06:12:49,560] [INFO] [utils.py:795:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-12-04 06:12:49,561] [INFO] [utils.py:796:see_memory_usage] MA 3.68 GB         Max_MA 3.68 GB         CA 3.88 GB         Max_CA 4 GB \n",
      "[2023-12-04 06:12:49,562] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 2.79 GB, percent = 9.0%\n",
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
      "[2023-12-04 06:12:51,743] [INFO] [utils.py:795:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-12-04 06:12:51,744] [INFO] [utils.py:796:see_memory_usage] MA 0.16 GB         Max_MA 3.68 GB         CA 3.88 GB         Max_CA 4 GB \n",
      "[2023-12-04 06:12:51,745] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 7.47 GB, percent = 24.1%\n",
      "[2023-12-04 06:12:51,746] [INFO] [config.py:979:print] DeepSpeedEngine configuration:\n",
      "[2023-12-04 06:12:51,747] [INFO] [config.py:983:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-12-04 06:12:51,747] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-12-04 06:12:51,748] [INFO] [config.py:983:print]   amp_enabled .................. False\n",
      "[2023-12-04 06:12:51,748] [INFO] [config.py:983:print]   amp_params ................... False\n",
      "[2023-12-04 06:12:51,749] [INFO] [config.py:983:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-12-04 06:12:51,749] [INFO] [config.py:983:print]   bfloat16_enabled ............. False\n",
      "[2023-12-04 06:12:51,749] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-12-04 06:12:51,750] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-12-04 06:12:51,750] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-12-04 06:12:51,751] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f14b87c6980>\n",
      "[2023-12-04 06:12:51,751] [INFO] [config.py:983:print]   communication_data_type ...... None\n",
      "[2023-12-04 06:12:51,751] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-12-04 06:12:51,752] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False\n",
      "[2023-12-04 06:12:51,754] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False\n",
      "[2023-12-04 06:12:51,754] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-12-04 06:12:51,754] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False\n",
      "[2023-12-04 06:12:51,755] [INFO] [config.py:983:print]   dataloader_drop_last ......... False\n",
      "[2023-12-04 06:12:51,755] [INFO] [config.py:983:print]   disable_allgather ............ False\n",
      "[2023-12-04 06:12:51,756] [INFO] [config.py:983:print]   dump_state ................... False\n",
      "[2023-12-04 06:12:51,756] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-12-04 06:12:51,757] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False\n",
      "[2023-12-04 06:12:51,757] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-12-04 06:12:51,758] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-12-04 06:12:51,759] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-12-04 06:12:51,759] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-12-04 06:12:51,759] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-12-04 06:12:51,760] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-12-04 06:12:51,761] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False\n",
      "[2023-12-04 06:12:51,761] [INFO] [config.py:983:print]   elasticity_enabled ........... False\n",
      "[2023-12-04 06:12:51,762] [INFO] [config.py:983:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-12-04 06:12:51,762] [INFO] [config.py:983:print]   fp16_auto_cast ............... None\n",
      "[2023-12-04 06:12:51,763] [INFO] [config.py:983:print]   fp16_enabled ................. False\n",
      "[2023-12-04 06:12:51,763] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-12-04 06:12:51,764] [INFO] [config.py:983:print]   global_rank .................. 0\n",
      "[2023-12-04 06:12:51,764] [INFO] [config.py:983:print]   grad_accum_dtype ............. None\n",
      "[2023-12-04 06:12:51,764] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 1\n",
      "[2023-12-04 06:12:51,765] [INFO] [config.py:983:print]   gradient_clipping ............ 0.0\n",
      "[2023-12-04 06:12:51,765] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-12-04 06:12:51,766] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-12-04 06:12:51,766] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-12-04 06:12:51,766] [INFO] [config.py:983:print]   load_universal_checkpoint .... False\n",
      "[2023-12-04 06:12:51,767] [INFO] [config.py:983:print]   loss_scale ................... 0\n",
      "[2023-12-04 06:12:51,767] [INFO] [config.py:983:print]   memory_breakdown ............. False\n",
      "[2023-12-04 06:12:51,768] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False\n",
      "[2023-12-04 06:12:51,768] [INFO] [config.py:983:print]   mics_shard_size .............. -1\n",
      "[2023-12-04 06:12:51,768] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-12-04 06:12:51,769] [INFO] [config.py:983:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-12-04 06:12:51,769] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-12-04 06:12:51,770] [INFO] [config.py:983:print]   optimizer_name ............... None\n",
      "[2023-12-04 06:12:51,770] [INFO] [config.py:983:print]   optimizer_params ............. None\n",
      "[2023-12-04 06:12:51,771] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2023-12-04 06:12:51,771] [INFO] [config.py:983:print]   pld_enabled .................. False\n",
      "[2023-12-04 06:12:51,771] [INFO] [config.py:983:print]   pld_params ................... False\n",
      "[2023-12-04 06:12:51,772] [INFO] [config.py:983:print]   prescale_gradients ........... False\n",
      "[2023-12-04 06:12:51,772] [INFO] [config.py:983:print]   scheduler_name ............... None\n",
      "[2023-12-04 06:12:51,773] [INFO] [config.py:983:print]   scheduler_params ............. None\n",
      "[2023-12-04 06:12:51,773] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2023-12-04 06:12:51,774] [INFO] [config.py:983:print]   sparse_attention ............. None\n",
      "[2023-12-04 06:12:51,774] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False\n",
      "[2023-12-04 06:12:51,774] [INFO] [config.py:983:print]   steps_per_print .............. 10\n",
      "[2023-12-04 06:12:51,775] [INFO] [config.py:983:print]   train_batch_size ............. 1\n",
      "[2023-12-04 06:12:51,775] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-12-04 06:12:51,776] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False\n",
      "[2023-12-04 06:12:51,776] [INFO] [config.py:983:print]   use_node_local_storage ....... False\n",
      "[2023-12-04 06:12:51,780] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False\n",
      "[2023-12-04 06:12:51,780] [INFO] [config.py:983:print]   weight_quantization_config ... q_type='symmetric' q_groups=1 enabled=True num_bits=8 quantized_initialization={'num_bits': 4, 'group_size': 64, 'group_dim': 1, 'symmetric': False} post_init_quant={}\n",
      "[2023-12-04 06:12:51,781] [INFO] [config.py:983:print]   world_size ................... 1\n",
      "[2023-12-04 06:12:51,781] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False\n",
      "[2023-12-04 06:12:51,782] [INFO] [config.py:983:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-12-04 06:12:51,783] [INFO] [config.py:983:print]   zero_enabled ................. True\n",
      "[2023-12-04 06:12:51,783] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-12-04 06:12:51,783] [INFO] [config.py:983:print]   zero_optimization_stage ...... 3\n",
      "[2023-12-04 06:12:51,784] [INFO] [config.py:969:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"train_batch_size\": 1, \n",
      "    \"weight_quantization\": {\n",
      "        \"quantized_initialization\": {\n",
      "            \"num_bits\": 4, \n",
      "            \"group_size\": 64, \n",
      "            \"group_dim\": 1, \n",
      "            \"symmetric\": false\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# run if using deepspeed\n",
    "model_engine, _, _, _ = deepspeed.initialize(config_params=ds_config, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if using deepspeed\n",
    "model_engine = model_engine.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir,\n",
    "        model_max_length=context_size if context_size > orig_ctx_len else orig_ctx_len,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(\n",
    "    model, tokenizer, temperature=0.6, top_p=0.9, max_gen_len=512, use_cache=True\n",
    "):\n",
    "    def response(prompt):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        #streamer = TextStreamer(tokenizer)\n",
    "        \n",
    "        # run if using deepspeed\n",
    "        output = model.module.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            use_cache=use_cache,\n",
    "            #streamer=streamer,\n",
    "        )\n",
    "        \n",
    "        # output = model.generate(\n",
    "        #     **inputs,\n",
    "        #     max_new_tokens=max_gen_len,\n",
    "        #     temperature=temperature,\n",
    "        #     top_p=top_p,\n",
    "        #     use_cache=use_cache,\n",
    "        #     #streamer=streamer,\n",
    "        # )\n",
    "        \n",
    "        out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        out = out.split(prompt.lstrip(\"<s>\"))[1].strip()\n",
    "        return out\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if using deepspeed\n",
    "respond = build_generator(model_engine, tokenizer, max_gen_len=512)\n",
    "# respond = build_generator(model, tokenizer, max_gen_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_no_input = PROMPT_DICT[\"prompt_llama2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Summarize the meeting transcript in two sentences.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_inference_mb(question, in_file, out_file):\n",
    "    with open(in_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for meeting in tqdm(data):\n",
    "            with torch.no_grad():\n",
    "                sample_result = {}\n",
    "                id = meeting[\"id\"]\n",
    "                material = meeting[\"conversations\"]\n",
    "                target = meeting[\"summary\"]\n",
    "                prompt = prompt_no_input.format_map({\"instruction\": material + \"\\n%s\"%question})\n",
    "                output = respond(prompt=prompt)\n",
    "                sample_result[\"target\"] = re.sub(r'\\n', '', target)\n",
    "                sample_result[\"prediction\"] = re.sub(r'\\n', '', output)\n",
    "                sample_result[\"id\"] = id\n",
    "                json.dump(sample_result, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "            del sample_result, material, target, prompt, output, id\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"../../QMSum/meetpeft/test_16k.json\"\n",
    "out_file = \"./output/output_QM_16k.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "100%|██████████| 20/20 [25:11<00:00, 75.56s/it]\n"
     ]
    }
   ],
   "source": [
    "file_inference_mb(question, in_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
