{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbb4ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from abc import ABC\n",
    "import datasets\n",
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "OPENAI_SECRET_KEY = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "656825a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_SECRET_KEY is None:\n",
    "  print(\"Please paste your OpenAI API key here:\")\n",
    "  OPENAI_SECRET_KEY = input().strip()\n",
    "openai.api_key = OPENAI_SECRET_KEY\n",
    "clear_output()\n",
    "\n",
    "class OpenAIEngine():\n",
    "  def __init__(self, model_name):\n",
    "    self.model_name = model_name\n",
    "\n",
    "  def score(self, text):\n",
    "    \"\"\"Tokenizes and scores a piece of text.\n",
    "\n",
    "    This only works for the OpenAI models which support the legacy `Completion`\n",
    "    API.\n",
    "\n",
    "    The score is log-likelihood. A higher score means a token was more\n",
    "    likely according to the model.\n",
    "\n",
    "    Returns a list of tokens and a list of scores.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=self.model_name,\n",
    "        prompt=text,\n",
    "        max_tokens=0,\n",
    "        logprobs=1,\n",
    "        echo=True)\n",
    "\n",
    "    tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
    "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
    "    if logprobs and logprobs[0] is None:\n",
    "      # GPT-3 API does not return logprob of the first token\n",
    "      logprobs[0] = 0.0\n",
    "    return tokens, logprobs\n",
    "\n",
    "  def perplexity(self, text):\n",
    "    \"\"\"Compute the perplexity of the provided text.\"\"\"\n",
    "    completion = openai.Completion.create(\n",
    "        model=self.model_name,\n",
    "        prompt=text,\n",
    "        logprobs=0,\n",
    "        max_tokens=0,\n",
    "        temperature=1.0,\n",
    "        echo=True)\n",
    "    token_logprobs = completion['choices'][0]['logprobs']['token_logprobs']\n",
    "    nll = np.mean([i for i in token_logprobs if i is not None])\n",
    "    ppl = np.exp(-nll)\n",
    "    return ppl\n",
    "\n",
    "  def generate(self,\n",
    "               prompt,\n",
    "               top_p=1.0,\n",
    "               num_tokens=32,\n",
    "               num_samples=1,\n",
    "               frequency_penalty=0.0,\n",
    "              presence_penalty=0.0):\n",
    "    \"\"\"Generates text given the provided prompt text.\n",
    "\n",
    "    This only works for the OpenAI models which support the legacy `Completion`\n",
    "    API.\n",
    "\n",
    "    If num_samples is 1, a single generated string is returned.\n",
    "    If num_samples > 1, a list of num_samples generated strings is returned.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "      engine=self.model_name,\n",
    "      prompt=prompt,\n",
    "      temperature=1.0,\n",
    "      max_tokens=num_tokens,\n",
    "      top_p=top_p,\n",
    "      n=num_samples,\n",
    "      frequency_penalty=frequency_penalty,\n",
    "      presence_penalty=presence_penalty,\n",
    "      logprobs=1,\n",
    "    )\n",
    "    outputs = [r[\"text\"] for r in response[\"choices\"]]\n",
    "    return outputs[0] if num_samples == 1 else outputs\n",
    "\n",
    "\n",
    "  def chat_generate(self,\n",
    "                    previous_messages,\n",
    "                    top_p=1.0,\n",
    "                    num_tokens=32,\n",
    "                    num_samples=1,\n",
    "                    frequency_penalty=0.0,\n",
    "                    presence_penalty=0.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=self.model_name,\n",
    "      messages=previous_messages,\n",
    "      temperature=1.0,\n",
    "      max_tokens=num_tokens,\n",
    "      top_p=top_p,\n",
    "      frequency_penalty=frequency_penalty,\n",
    "      presence_penalty=presence_penalty,\n",
    "      n=num_samples,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c703e78",
   "metadata": {},
   "source": [
    "# MeetingBank dataset:\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aaa967",
   "metadata": {},
   "source": [
    "### GPT3-D3 zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2af024",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../../MeetingBank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fbff76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker 0: Great. Thank you. Next item, please.\n",
      "speaker 1: Item 27 Report from Economic Development. Recommendation to execute a Second Amendment to Exclusive Negotiation Agreement with Howard CDB for the continuation of negotiations in connection with the proposed development of the former Long Beach Armory at 854 East Seventh Street District one.\n",
      "speaker 0: Can have a second, please. Thank you, Mr. Modica. Just just briefly, because I'm I'm very personally very interested in this project. I think it's a phenomenal opportunity and we've been discussing it for for many years, and it seems to be in a very good place.\n",
      "speaker 0: Can you can Stafford provide a just a brief update on this?\n",
      "speaker 3: Yes, John Keisler. Honorable mayor and members of the city council. It's so good to see you. And I'm going to ask our deputy director, Sergio Ramirez, to give a short staff report about this amazing project. Thank you, John.\n",
      "speaker 3: Good evening, honorable mayor and council members. We have actually been working with Country Partners, a partnership between Howard Cdmo, Saint Anthony and Pacific six on the adaptive reuse of the former Armory Building, which would consist of the Adaptive Reuse Project and a new development component back in February entered into an inner with the developers and have been\n",
      "speaker 3: working with them. They are. We've been working on several items and have negotiations gone very well. They are actually applying for a state and for infrastructure grant and that's part of the grant and requires them to have additional time and staff is supportive of the extension and we will continue to work with them to hopefully bring forward\n",
      "speaker 3: to you a definitive project agreement. And that hopefully request.\n",
      "speaker 0: Support for this item. Thank you. Very supportive of this. I appreciate the update conference and discuss an article.\n",
      "speaker 5: Thank you, Mary. Thank you. Thank you so much to both John and Sergio who have been working really hard on this item. I'm I'm super excited to continue seeing the partnership with Howard CDM and so happy to know that the proposed development would be bringing in 100% affordable housing projects right here in in my district, in the\n",
      "speaker 5: first district. As I mentioned, finding creative ways to address our housing crisis and supporting affordable housing projects is altogether one of my greatest priorities as a councilwoman. So I'm so happy to be able to support this item. Thank you for all the hard work that has been put into this.\n",
      "speaker 5: And reviving the armory is just going to be something phenomenal, not just for the First District, but for the city as a home.\n",
      "speaker 0: Thank you, Councilwoman Councilor, for your comments.\n",
      "speaker 2: It's a great project. I support it wholeheartedly.\n",
      "speaker 0: And I don't believe there is public comment on this.\n",
      "speaker 1: Not on this item.\n",
      "speaker 0: Please cast your votes. What's coming up? There's a revolt. Thank you. Don't forget to vote, guys. Okay. Motion carries. Thank you. Next item, please. I am 30.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "val_path = os.path.join(data_folder, \"test.json\")\n",
    "with open(val_path, 'r') as f:\n",
    "    val_data = json.load(f)\n",
    "print(val_data[20]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705d0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "from abc import ABC\n",
    "import datasets\n",
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "OPENAI_SECRET_KEY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2236104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_SECRET_KEY is None:\n",
    "  print(\"Please paste your OpenAI API key here:\")\n",
    "  OPENAI_SECRET_KEY = input().strip()\n",
    "openai.api_key = OPENAI_SECRET_KEY\n",
    "clear_output()\n",
    "\n",
    "class OpenAIEngine():\n",
    "  def __init__(self, model_name):\n",
    "    self.model_name = model_name\n",
    "\n",
    "  def score(self, text):\n",
    "    \"\"\"Tokenizes and scores a piece of text.\n",
    "\n",
    "    This only works for the OpenAI models which support the legacy `Completion`\n",
    "    API.\n",
    "\n",
    "    The score is log-likelihood. A higher score means a token was more\n",
    "    likely according to the model.\n",
    "\n",
    "    Returns a list of tokens and a list of scores.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=self.model_name,\n",
    "        prompt=text,\n",
    "        max_tokens=0,\n",
    "        logprobs=1,\n",
    "        echo=True)\n",
    "\n",
    "    tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
    "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
    "    if logprobs and logprobs[0] is None:\n",
    "      # GPT-3 API does not return logprob of the first token\n",
    "      logprobs[0] = 0.0\n",
    "    return tokens, logprobs\n",
    "\n",
    "  def perplexity(self, text):\n",
    "    \"\"\"Compute the perplexity of the provided text.\"\"\"\n",
    "    completion = openai.Completion.create(\n",
    "        model=self.model_name,\n",
    "        prompt=text,\n",
    "        logprobs=0,\n",
    "        max_tokens=0,\n",
    "        temperature=1.0,\n",
    "        echo=True)\n",
    "    token_logprobs = completion['choices'][0]['logprobs']['token_logprobs']\n",
    "    nll = np.mean([i for i in token_logprobs if i is not None])\n",
    "    ppl = np.exp(-nll)\n",
    "    return ppl\n",
    "\n",
    "  def generate(self,\n",
    "               prompt,\n",
    "               top_p = 1):\n",
    "    \"\"\"Generates text given the provided prompt text.\n",
    "\n",
    "    This only works for the OpenAI models which support the legacy `Completion`\n",
    "    API.\n",
    "\n",
    "    If num_samples is 1, a single generated string is returned.\n",
    "    If num_samples > 1, a list of num_samples generated strings is returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=self.model_name,\n",
    "      messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "      top_p=top_p\n",
    "    \n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "  def chat_generate(self,\n",
    "                    previous_messages,\n",
    "                    top_p=1.0,\n",
    "                    num_tokens=32,\n",
    "                    num_samples=1,\n",
    "                    frequency_penalty=0.0,\n",
    "                    presence_penalty=0.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=self.model_name,\n",
    "      messages=previous_messages,\n",
    "      temperature=1.0,\n",
    "      max_tokens=num_tokens,\n",
    "      top_p=top_p,\n",
    "      frequency_penalty=frequency_penalty,\n",
    "      presence_penalty=presence_penalty,\n",
    "      n=num_samples,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8316d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff03e7b8",
   "metadata": {},
   "source": [
    "## Calculate openai cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1c9aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 862/862 [00:00<00:00, 300664.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cost: 60.59104199999995 dollars (excluding output cost)\n",
      "Seq <4000: 637,0.7389791183294664%\n",
      "Seq <16000: 188,0.21809744779582366%\n",
      "Seq <32000: 25,0.029002320185614848%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#val data loaded, now use openai api to get summary sample by sample\n",
    "#the result will be writen to a json for later evaluation\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "results = []\n",
    "total_cost = 0\n",
    "len_stat = [0,0,0]\n",
    "for sample in tqdm(val_data):\n",
    "    prompt = f\"{sample['source']}\\nSummarize the above article in 2 sentences.\"\n",
    "    _seq_len = len(prompt)/4\n",
    "    if _seq_len<4000:\n",
    "        total_cost+=0.0015*_seq_len/1000\n",
    "        len_stat[0]+=1\n",
    "    elif _seq_len<16000:\n",
    "        total_cost+=0.003*_seq_len/1000\n",
    "        len_stat[1]+=1\n",
    "    elif _seq_len<32000:\n",
    "        total_cost+=0.06*_seq_len/1000\n",
    "        len_stat[2]+=1\n",
    "    else:\n",
    "        total_cost+=0.06*32000/1000\n",
    "\n",
    "print(f\"total cost: {total_cost} dollars (excluding output cost)\")\n",
    "print(f\"Seq <4000: {len_stat[0]},{len_stat[0]/len(val_data)}%\")\n",
    "print(f\"Seq <16000: {len_stat[1]},{len_stat[1]/len(val_data)}%\")\n",
    "print(f\"Seq <32000: {len_stat[2]},{len_stat[2]/len(val_data)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da8c602",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf71ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-3.5-turbo-16k\"\n",
    "engine = OpenAIEngine(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54daae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/861 [01:40<1:10:42,  5.04s/it]\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 16385 tokens. However, your messages resulted in 16933 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sample_result[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sample[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m sample_result[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, sample[\u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m#remove all \\n\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m generation \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mgenerate(prompt, top_p\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m sample_result[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, generation)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m results\u001b[39m.\u001b[39mappend(sample_result)\n",
      "\u001b[1;32m/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb 单元格 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m              prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m              top_p \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Generates text given the provided prompt text.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m  This only works for the OpenAI models which support the legacy `Completion`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m  If num_samples > 1, a list of num_samples generated strings is returned.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m   response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt}],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m   \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m   )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhuzengliang/Documents/GitHub/MeetPEFT/Benchmarks/GPT/benchmark_gpt.ipynb#W5sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 16385 tokens. However, your messages resulted in 16933 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "\n",
    "#val data loaded, now use openai api to get summary sample by sample\n",
    "#the result will be writen to a json for later evaluation\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "results = []\n",
    "for sample in tqdm(val_data):\n",
    "    prompt = f\"{sample['source']}\\nSummarize the above article in 2 sentences.\"\n",
    "    sample_result = dict()\n",
    "    sample_result[\"id\"] = sample[\"id\"]\n",
    "    sample_result[\"target\"] = output_text = re.sub(r'\\n', '', sample[\"summary\"]) #remove all \\n\n",
    "    generation = engine.generate(prompt, top_p=0.5)\n",
    "    sample_result[\"prediction\"] = re.sub(r'\\n', '', generation)\n",
    "    results.append(sample_result)\n",
    "with open(\"output.json\", \"w\") as json_file:\n",
    "    for item in results:\n",
    "        json.dump(item, json_file)\n",
    "        json_file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b4952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcds",
   "language": "python",
   "name": "fcds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
