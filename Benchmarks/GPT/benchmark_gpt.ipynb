{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89e462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhuzengliang/.local/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from abc import ABC\n",
    "import datasets\n",
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "OPENAI_SECRET_KEY = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6029a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_SECRET_KEY is None:\n",
    "  print(\"Please paste your OpenAI API key here:\")\n",
    "  OPENAI_SECRET_KEY = input().strip()\n",
    "openai.api_key = OPENAI_SECRET_KEY\n",
    "clear_output()\n",
    "\n",
    "class OpenAIEngine():\n",
    "  def __init__(self, model_name):\n",
    "    self.model_name = model_name\n",
    "\n",
    "  def score(self, text):\n",
    "    \"\"\"Tokenizes and scores a piece of text.\n",
    "\n",
    "    This only works for the OpenAI models which support the legacy `Completion`\n",
    "    API.\n",
    "\n",
    "    The score is log-likelihood. A higher score means a token was more\n",
    "    likely according to the model.\n",
    "\n",
    "    Returns a list of tokens and a list of scores.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=self.model_name,\n",
    "        prompt=text,\n",
    "        max_tokens=0,\n",
    "        logprobs=1,\n",
    "        echo=True)\n",
    "\n",
    "    tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
    "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
    "    if logprobs and logprobs[0] is None:\n",
    "      # GPT-3 API does not return logprob of the first token\n",
    "      logprobs[0] = 0.0\n",
    "    return tokens, logprobs\n",
    "\n",
    "  def perplexity(self, text):\n",
    "    \"\"\"Compute the perplexity of the provided text.\"\"\"\n",
    "    completion = openai.Completion.create(\n",
    "        model=self.model_name,\n",
    "        prompt=text,\n",
    "        logprobs=0,\n",
    "        max_tokens=0,\n",
    "        temperature=1.0,\n",
    "        echo=True)\n",
    "    token_logprobs = completion['choices'][0]['logprobs']['token_logprobs']\n",
    "    nll = np.mean([i for i in token_logprobs if i is not None])\n",
    "    ppl = np.exp(-nll)\n",
    "    return ppl\n",
    "\n",
    "  def generate(self,\n",
    "               prompt,\n",
    "               top_p=1.0,\n",
    "               num_tokens=32,\n",
    "               num_samples=1,\n",
    "               frequency_penalty=0.0,\n",
    "              presence_penalty=0.0):\n",
    "    \"\"\"Generates text given the provided prompt text.\n",
    "\n",
    "    This only works for the OpenAI models which support the legacy `Completion`\n",
    "    API.\n",
    "\n",
    "    If num_samples is 1, a single generated string is returned.\n",
    "    If num_samples > 1, a list of num_samples generated strings is returned.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "      engine=self.model_name,\n",
    "      prompt=prompt,\n",
    "      temperature=1.0,\n",
    "      max_tokens=num_tokens,\n",
    "      top_p=top_p,\n",
    "      n=num_samples,\n",
    "      frequency_penalty=frequency_penalty,\n",
    "      presence_penalty=presence_penalty,\n",
    "      logprobs=1,\n",
    "    )\n",
    "    outputs = [r[\"text\"] for r in response[\"choices\"]]\n",
    "    return outputs[0] if num_samples == 1 else outputs\n",
    "\n",
    "\n",
    "  def chat_generate(self,\n",
    "                    previous_messages,\n",
    "                    top_p=1.0,\n",
    "                    num_tokens=32,\n",
    "                    num_samples=1,\n",
    "                    frequency_penalty=0.0,\n",
    "                    presence_penalty=0.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=self.model_name,\n",
    "      messages=previous_messages,\n",
    "      temperature=1.0,\n",
    "      max_tokens=num_tokens,\n",
    "      top_p=top_p,\n",
    "      frequency_penalty=frequency_penalty,\n",
    "      presence_penalty=presence_penalty,\n",
    "      n=num_samples,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0d3a5",
   "metadata": {},
   "source": [
    "# MeetingBank dataset:\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c2b06",
   "metadata": {},
   "source": [
    "### GPT3-D3 zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4cf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcds",
   "language": "python",
   "name": "fcds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
